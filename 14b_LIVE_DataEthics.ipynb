{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "69d4359d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Fundamental Concepts in Data Insight: \n",
    "## <font color=indigo> Data Ethics </font>\n",
    "\n",
    "### Fundamentals for a General Audience\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fb17177",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "QA Ltd. owns the copyright and other intellectual property rights of this material and asserts its moral rights as the author. All rights reserved."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc16b8ec",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### **Data Ethics**\n",
    "* Why Data Ethics?\n",
    "    * What is Data Ethics?\n",
    "* Policing Case Studies: Motivations\n",
    "    * Data Storage\n",
    "    * Illegal Actions \n",
    "    * Bias \n",
    "* Mini Case Studies: Motivations & Problems\n",
    "    * Case Study: Tesla Crash (Responsibility)\n",
    "    * Case Study: `care.data` (Importance of Ethics)\n",
    "    * Case Study: Airport Threat Detection (Risks)\n",
    "    * Case Study: Unintended behaviour\n",
    "    * Case Study: Street Bump (Inequality)\n",
    "* Topics in Data Ethics\n",
    "    * Ethical Systems\n",
    "    * The Ethics of (Opaque) Algorithms\n",
    "    * Problem in Data Ethics\n",
    "        * Problems: Privacy\n",
    "        * Problems: Discrimination\n",
    "        * Problems: Auditing & Responsibility\n",
    "* Topic Focus: Accountability & Explanation\n",
    "    * Case Study: Compass Recidivisism System\n",
    "    * Why don't we get explanations?\n",
    "    * What is an Explanation of a Model?\n",
    "    * What is an Explanation?\n",
    "    * Counterfactual explanations of models\n",
    "* Group Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acbd7709",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Why Data Ethics?\n",
    "* ethical implications of data analysis\n",
    "    * we need algorithmic systems to process data\n",
    "    * novel practices requried, driven by data\n",
    "    \n",
    "* data ethics concerned with data itself\n",
    "    * producing large volumes of data\n",
    "    \n",
    "* data combined with other sources \n",
    "    * suprising cross-domain inferences \n",
    "* concerns\n",
    "    * fairness, responsibility, respect for HR\n",
    "    * counter-productive ot ignore\n",
    "        * back-reactions & downsides\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76b05f8e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## What is Data Ethics?\n",
    "\n",
    "* ethics of data\n",
    "    * privacy\n",
    "    * trust\n",
    "    * transparency\n",
    "* ethics of algorithms\n",
    "    * accountability\n",
    "    * design\n",
    "    * auditing\n",
    "* ethics of practices\n",
    "    * deontological code\n",
    "    * consent \n",
    "    * privacy\n",
    "      \n",
    "* laws\n",
    "    * seperate legal structures\n",
    "    * smart robotics, AI, data protection\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e99fbc6e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Policing Case Studies: Motivations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc7a9741",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Data Storage\n",
    "\n",
    "> Investment in databases and their functionalities has met opposition from oversight commission, human rights, and public interest groups. In the U.K “The Biometrics Commissioner has warned [the police] that **many of the 20 million custody photographs currently stored on their systems are being held unlawfully** and might need to be destroyed” (Loeb, 2018). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcc42d37",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "> **Merging the three databases into the NLEDS, not only accumulates these existing concerns it also creates new ones related** to proportionality, legitimacy, and ownership of data. While “proportionality will be a design feature of the system with permission-based access, with a full audit trail and a description of purpose of access. There is much work to do in terms of exact detail” (Surveillance Camera Commissioner, 2016: 23). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90ef8887",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Illegal Actions \n",
    "\n",
    "> A clear example of this was during the **G20 summit in Hamburg**, Germany when the **accreditation of 32 journalists was revoked as a result of them being labeled as ‘left motivated violent offenders’** in a BKA database (Monroy et al, 2018). **Legal challenges revealed that the decision to revoke their accreditation was based on ‘old’ data which was never deleted**. According to the BKA, it is the responsibility of the authorities, who enter data on suspects into the system, to delete old records. In most cases, data entry is done by the German states, which fail to remove ‘old’ data (Fiedler, 2017)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49caa807",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "> In Cardiff, the South Wales Police received £2.6m from the Police Transformation\n",
    "Fund to lead on the testing and deployment of AFR, and in some instances LFR. **“South Wales Police has admitted it has used AFR technology to target** petty criminals, such as ticket touts and pickpockets outside football matches, but they have also used it on **peaceful protesters**” (Liberty, 2018). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6d2f6b3",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Bias \n",
    "> **The AFR systems used in by U.K police were found to perform abdominally poorly, with “on average, a staggering 95% of ‘matches’ wrongly identified innocent people”** (Ferris, 2018: 13), who were subsequently **stopped and asked to prove their identity**. These findings of flawed facial recognition technology are in line with research showing **inherent bias in facial recognition** systems towards women and people of colour (Buolamwini et all, 2018; Ferris, 2018). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9608610",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "> **The London MET developed the Gang Matrix to identify potential gang members** and score them according to the risk they pose to society. Research by Amnesty International (2018) and Scott (2018) revealed the discriminatory nature of this predictive identification program, in which **the majority of individuals were young black men**. The MET was ordered to radically reform the matrix within a year by the Mayor of London, and are currently working on a new program called the ‘Concern Hub’ (Mayor or London, 2018a: Dodd, 2018; Crisp, 2019)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbe63c36",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "> The use of predictive policing technologies has raised many concerns. Scholars in the USA have demonstrated how **historically biased police data** in predictive policing programs is **erpetuating the over-policing of African American neighbourhoods** (Lum et al, 2016). In the U.K Liberty found a similar negative feedback loop, people from \"back, Asian and minority ethnic (BAME) communities are disproportionately more likely to be arrested, leading the program to assume, wrongly that the area in which they live or spend time are the areas where there is more crime” (Couchman, 2019: 4). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2745475c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Mini Case Studies: Motivations & Problems"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9279056f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Case Study: Tesla Crash (Responsibility)\n",
    "* tesla in autopiolt mode\n",
    "    * who is responsible for crash?\n",
    "* oversight is very difficult\n",
    "    * many parts of systems\n",
    "* \"distributed responsibility\"\n",
    "    * how does liability apply to sofwtare?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "947247e0",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Case Study: `care.data` (Importance of Ethics)\n",
    "* https://en.wikipedia.org/wiki/Care.data\n",
    "* sharing health data in UK\n",
    "* limited consent\n",
    "* didnt go ahead"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78209fa8",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Case Study: Airport Threat Detection (Risks)\n",
    "* robot patrol or CCTV\n",
    "* people suffer harm from these systems, ie., detention\n",
    "* who can you hold to account for this?\n",
    "* where is redress?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad294321",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Case Study: Unintended behaviour\n",
    "* (hidden) competition between wikiepdeia bots\n",
    "    * unexpected interactions\n",
    "* high-frequency trading crashes\n",
    "* tay chatbot \n",
    "    * effect of *who* was training system!\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5df2345",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Case Study: Street Bump (Inequality)\n",
    "* areas with smartphones could register potholes\n",
    "    * so those areas got fixed first -- exagerate inequality \n",
    "* app could have used smartphones as *a* source of data\n",
    "    * but it was *the* source, and so exclusionary "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "059314fc",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46f67a5e",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## Topics in Data Ethics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72f3d0b9",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfd4057f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Ethical Systems"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5857881f",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* denotology = principles = motivations & intentions\n",
    "    * autonomy = ends in themselves\n",
    "    * procedural justice \n",
    "        * do no harm\n",
    "    * not complete, eg., \"unintended concequences\"\n",
    "* concequentialism = outcomes, effects\n",
    "    * better for \n",
    "        * unitended concequences\n",
    "        * broader impact of data sci\n",
    "    * also limited\n",
    "        * can over-look effects on invididuals\n",
    "* \"environmental approach\"\n",
    "    * respect & care are due to \" an environment \"\n",
    "    * individual duties to protect and foster\n",
    "    * gardners?\n",
    "    * trying to combine deont + concq. \n",
    "* virute ethics\n",
    "* causitry "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "041dd1a6",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The Ethics of (Opaque) Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb8fc4ac",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* data-processing decision-making algorithms\n",
    "    * specification\n",
    "    * implementation\n",
    "    * configuration\n",
    "\n",
    "* concerns\n",
    "    * evidence is\n",
    "        * inconcluive (unjustified)\n",
    "        * inscruitable (unexplainable, opacity)\n",
    "        * misguided (stat. biased)\n",
    "    * unfair outcomes  (morally biased, discrimination)\n",
    "    * transformative effects (autonomy, privacy)\n",
    "    * traceability (audit/responsibility)\n",
    "        * de-respsonsibilisation\n",
    "            * \"computer says so\"\n",
    "            \n",
    "* when is correlation sufficinety reliable?\n",
    "    * limit use of opaque methods in some contexts?\n",
    "        * credit, policing,...\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0de0c15",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4138cdb",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## Problem in Data Ethics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fb6917b",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43b7e59a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Problems: Privacy\n",
    "* EU data protection\n",
    "    * identifiable individuals \n",
    "        * groups are not protected\n",
    "        * \"consent\"\n",
    "    * remove/hide in datasets\n",
    "        * within law, individuals lose control over anonymised data\n",
    "    * however,\n",
    "        * in ml/pa you are grouped into sets of similar people\n",
    "        * their actions affect you\n",
    "        * these are ad-hoc groupings based on organizations' interest\n",
    "            * not collective orgs or ascriptive\n",
    "                * eg., union, genetic group\n",
    "    * a group privacy right may be required\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6d7db3f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "    \n",
    "## Problems: Discrimination\n",
    "* conclusive, transparent, well-founded (automated) decisions can be discimrinative\n",
    "* bias *in human labelling!* of training data\n",
    "* \"racial\" correaltions found *after*wards"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c29ce9e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Problems: Auditing & Responsibility\n",
    "* who makes the ethical choice?\n",
    "    * auto-car: driver, machine?\n",
    "        * moral effects obvious\n",
    "    * many systems have non-obvious moral effects\n",
    "    * organizational policy...\n",
    "        * programmer?\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "571a3fff",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9e77a0a",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## Topic Focus: Accountability & Explanation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b12022b8",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4439bd1c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Case Study: Compass Recidivisism System\n",
    "\n",
    "* More false-positive for black defendents than for white defendents. \n",
    "\n",
    "* Related court case\n",
    "    * Wisconsin vs. .... -- do you have a right to an explanation?\n",
    "    * judge said: system is trade secret!?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "981ff520",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "##  Why don't we get explanations?\n",
    "* infringe trade secrets\n",
    "* technically infeasible\n",
    "* not meaningful for individual cases\n",
    "* manipulation of system by third parties\n",
    "    * eg., Tay chatbot\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03aa53df",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## What is an Explanation of a Model?\n",
    "\n",
    "* holistic or local explanantion\n",
    "    * how model works in general?\n",
    "    * or your partiuclar decision?\n",
    "* form of explanation\n",
    "    * visual\n",
    "    * audience: child, adult, expert\n",
    "* what innformation?\n",
    "    * source code\n",
    "    * input data\n",
    "    * weights\n",
    "    * model\n",
    "    * ...?\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b101ec89",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## What is an Explanation?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c259486f",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* number of ways of providing explanations...\n",
    "    * case-based explanations\n",
    "        * find a clear historical case similar to current one\n",
    "        * provide that justification\n",
    "    * local explanations\n",
    "        * train a simpler model within reigion of complex model (eg., linear)\n",
    "        * eg., \"How do I drive north?\"\n",
    "            * forwards *only*? or to a destination?\n",
    "    * counter-factual explanations\n",
    "        * understand, challenge or alter a decision\n",
    "        * if q were false, S would not believe p\n",
    "            * so q *explains* \"belief that p\"\n",
    "        * multiple counterfactual explanations are possible\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26437f91",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Counterfactual explanations of models\n",
    "* (credit score) pv was returned because \n",
    "    * variables V\n",
    "    * had values v1, ..vn\n",
    "* if they had instead had U (u1..un)\n",
    "    * score pu would be returned\n",
    "\n",
    "* eg., you were denied a loan because your income was £35k, if your income was £45k then you would have been offered a loan\n",
    "\n",
    "* can be used with blacbox algs\n",
    "    * you can \"find nearest\" input variable values to arrive at desired output, ie., alternative decision"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bdeb210",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Group Exercise\n",
    "\n",
    "* consider one of the case studies where you were a victim of an automated decision\n",
    "* what information would you want? why?\n",
    "* what would constitute a meaningful explanation?\n",
    "* does this change depending on who you are?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21cde461",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89035413",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The Law"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6a790f5",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## Concerns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a3c6e87",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "#### Changing Laws\n",
    "> In 2019 the German ANPR systems became contested, the German Constitutional Court ruled that mass registration of license plates infringes on the right to informational self-determination and limits individual freedoms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93752137",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### International Regulartory Concerns\n",
    "\n",
    "> Police forces across Europe are mainly turning to Palantir , a big data analytics company from the US with a track record of contracting for police and secret services across the world, for the integration and searching of heterogeneous databases, and social graphing of suspects\n",
    "\n",
    "> European human rights groups, oversight committees, and scholars have been less vocal on the use of Palantir technologies by European police forces then they have been on the creation of databases, real time identification, and predictive policing systems. Concerns have been raised on the use of an American company for crunching French police data (Samama, 2018), how the use of Palantir software might not be in-line with national and European data protection regulation (EDRi, 2017; Technology World, 2018) or more fundamentally how the use of Hessendata by police to track suspects blurs the separation between the German police and secret service (Kurz, 2018). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9bc0e2f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## GDPR\n",
    "\n",
    "\n",
    "*does not provide a right to decision making!*\n",
    "\n",
    "you have a right to receive *some* information, and what *type* of data is used, and what *of your data* is being used\n",
    "\n",
    "but not right to a decision on your individual decision. \n",
    "\n",
    "a \"right to be informed\" vs. a right to an explanation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7953c9e3",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Data Protection Directive (Not Legally Binding, presently) -- Art 12a\n",
    "\n",
    "* right to obtain informaton from controller\n",
    "    * \"knowledge of the logic involved in any automatic processing\"\n",
    "    \n",
    "    \n",
    "* judges have concluded: very lmimted\n",
    "    * broad overview, no detail on tradescretes\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "577ee331",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Art 22 GDPR -- Automated Individual Decision-Making\n",
    "\n",
    "\n",
    "teh data subject shall have the right..\n",
    "\n",
    "\n",
    "\n",
    "**right to contest** -- meaningless\n",
    "\n",
    "\n",
    "* **not explanation** of the *rational*\n",
    "\n",
    "\n",
    "only for *soley* automated -- any human involvement drops right; only where *significant effects* (undefined("
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
